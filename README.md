Employee Turnover Prediction â€” Machine Learning Project
This project explores employee turnover using real-world HR data and applies machine-learning models to predict which employees are most likely to leave a company.
It includes Exploratory Data Analysis (EDA), clustering, class imbalance handling, and performance comparison of three classification models: Logistic Regression, Random Forest, and Gradient Boosting.
ğŸ“Š Project Overview
Companies face high costs when losing trained employees. The goal of this project is to:
Understand what factors contribute to turnover
Identify behavior patterns among employees who left
Build ML models that accurately predict employee churn
Use prediction probabilities to categorize employees into actionable risk groups
Dataset source: Modified from Kaggle â€” HR Analytics: Employee Turnover.
ğŸ” Exploratory Data Analysis (EDA)
Key findings from EDA:
Satisfaction level shows the strongest negative correlation with turnover.
Employees with high evaluation scores but low satisfaction frequently leftâ€”suggesting overwork or insufficient compensation.
Monthly working hours peak between 160â€“270 hours, indicating potential workload strains.
Number of projects was not a strong standalone predictor.
Visualizations included:
Correlation heatmap
Distribution plots
Bar plot of project count by turnover status
ğŸ§­ Clustering Analysis
K-Means was applied to employees who left to understand different patterns among them:
Cluster 2: High evaluation + low satisfaction â†’ likely overworked high performers
Cluster 1: Medium satisfaction + low evaluation â†’ possibly disengaged
Cluster 2: High satisfaction + high evaluation â†’ may have found better opportunities
Salary was later encoded and scaled for consistency during clustering.
âš–ï¸ Handling Imbalanced Data
Because only a minority of employees leave, the dataset was imbalanced.
To address this, SMOTE (Synthetic Minority Oversampling Technique) was used to upsample the minority class in the training data, ensuring fair model training.
ğŸ¤– Models Trained
Three classification models were built and compared using cross-validation, ROC/AUC, and confusion matrices:
1. Logistic Regression
Accuracy: ~74%
Strong for baseline interpretation
Lower recall for employees who left
2. Random Forest
Accuracy: ~93%
Strong precision and recall
Good at capturing nonlinear relationships
3. Gradient Boosting (Best Model)
Accuracy: ~96%
Highest recall and f1-score for the â€œleftâ€ class
Best overall model for identifying at-risk employees
ğŸ“ˆ Why Evaluation Metrics Matter
Accuracy alone is misleading because â€œleftâ€ cases are rare.
A model might achieve high accuracy simply by predicting that everyone stays.
Key metrics used:
Precision â€“ Of predicted leavers, how many truly left?
Recall â€“ How many of all true leavers the model successfully identified.
F1-score â€“ Balance between precision and recall
ROC/AUC â€“ Overall model discriminative ability
Recall is emphasized, because failing to identify a true leaver is costlier for HR than flagging a possible false alarm.
ğŸš¦ Turnover Risk Categorization
Using the best model (Gradient Boosting), employees were classified into:
Safe Zone (<20%)
Low Risk (20â€“40%)
Moderate Risk (40â€“60%)
High Risk (>60%)
This helps HR teams intervene early.
ğŸ§© Retention Strategy Suggestions
Based on model insights:
Reduce excessive working hours
Limit the number of simultaneous projects
Improve compensation for high-performing employees at risk of burnout
ğŸ—‚ï¸ Repository Contents
main.py â€” Project notebook
employee_data_hr_comma_sep.xlsx â€” Dataset (if included)
Python scripts for preprocessing, modeling, and evaluation
Visualizations and model evaluation outputs
